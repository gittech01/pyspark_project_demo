{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cda4a05f-11af-4c18-ae34-b5a78d7a134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8698443c-6749-44ae-a28d-62c3a208802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparkSession(name_app: str, core_limit: int = None) -> SparkSession:\n",
    "    \n",
    "    core = '*' if not core_limit else core_limit\n",
    "    \n",
    "    spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .master(f'local[{core}]')\n",
    "        .appName(name_app)\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d507a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(file_name: str) -> list:\n",
    "    import glob\n",
    "    \n",
    "    files = glob.glob(f\"{file_name}/*.json\")\n",
    "    return files\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35fe79db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_json_spark(spark_session: SparkSession, path_file: list | str) -> list[DataFrame]:\n",
    "    list_spark_frames: list = []\n",
    "\n",
    "    path_files = [path_file] if not isinstance(path_file, list) else path_file\n",
    "\n",
    "    for path in path_files:\n",
    "        frame = spark_session.read.option(\"multiline\",\"true\").json(path)\n",
    "        list_spark_frames.append(frame.select(\"name\", \"packages\"))\n",
    "\n",
    "    return list_spark_frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fe067dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_explode_columns_spark_frame(dataframe: DataFrame) -> DataFrame:\n",
    "    \n",
    "    select_explode_dataframe = (\n",
    "            dataframe\n",
    "            .select(\n",
    "                F.col(\"name\").alias(\"new_name\"),\n",
    "                F.explode(F.col(\"packages\")).alias(\"pacotes\")\n",
    "            )\n",
    "            .select(\"new_name\", \"pacotes.*\")\n",
    "        )\n",
    "    return select_explode_dataframe, select_explode_dataframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ae57a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_select_columns_spark_frame(dataframe: DataFrame, cols: list) -> DataFrame:\n",
    "    COLS = [\"name\", \"versionInfo\", \"licenseConcluded\"]\n",
    "    \n",
    "    for col in COLS:\n",
    "        new_name_col = (\n",
    "                    \"new_biblioteca\" if col == COLS[0]\n",
    "            else \"versao_biblioteca\" if col == COLS[1]\n",
    "            else \"licenca_biblioteca\"\n",
    "        )\n",
    "        if col not in cols:\n",
    "            dataframe = dataframe.withColumn(new_name_col, F.lit(None))\n",
    "            continue\n",
    "        dataframe = dataframe.withColumnRenamed(col, new_name_col)\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf54f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_multiples_spark_frame(list_spark_frames: list) -> DataFrame:\n",
    "\n",
    "    if len(list_spark_frames) == 1:\n",
    "        return list_spark_frames[0]\n",
    "    \n",
    "    final_frame = list_spark_frames[0]\n",
    "    for frame in list_spark_frames[1:]:\n",
    "        final_frame = final_frame.union(frame).distinct()\n",
    "    return final_frame      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bb75408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_spark_frame(list_spark_frames: list) -> DataFrame:\n",
    "    \n",
    "    result_frames = []\n",
    "    for spark_frame in list_spark_frames:\n",
    "        dataframe, cols = select_explode_columns_spark_frame(spark_frame)\n",
    "        dataframe = create_select_columns_spark_frame(dataframe, cols)\n",
    "\n",
    "        dataframe = dataframe.select(\n",
    "                        F.col(\"new_name\")\n",
    "                        , F.col(\"new_biblioteca\")\n",
    "                        , F.col(\"versao_biblioteca\")\n",
    "                        , F.col(\"licenca_biblioteca\")\n",
    "                    )\n",
    "        \n",
    "        frame = dataframe.filter(dataframe.new_name != dataframe.new_biblioteca)\n",
    "        result_frames.append(frame)\n",
    "    final_dataframe = union_multiples_spark_frame(result_frames)\n",
    "    \n",
    "    final_dataframe = final_dataframe.withColumn(\"nome_repositorio\", F.split(F.col(\"new_name\"), '/')[1])\n",
    "    final_dataframe = final_dataframe.withColumn(\"gerenciador_biblioteca\", F.split(F.col(\"new_biblioteca\"), ':')[0])\n",
    "    final_dataframe = final_dataframe.withColumn(\"nome_biblioteca\", F.split(F.col(\"new_biblioteca\"), ':')[1])\n",
    "\n",
    "    final_dataframe = (\n",
    "            final_dataframe.\n",
    "            select(*[col for col in final_dataframe.columns if col not in ('new_name','new_biblioteca')])\n",
    "        )\n",
    "    \n",
    "    return final_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2b90ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/26 10:14:19 WARN Utils: Your hostname, edsojor resolves to a loopback address: 127.0.1.1; using 192.168.15.6 instead (on interface wlo1)\n",
      "24/06/26 10:14:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/06/26 10:14:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_session = sparkSession(name_app='Sbom', core_limit=2)\n",
    "spark_session.sparkContext.setLogLevel(\"ERROR\") \n",
    "\n",
    "\n",
    "## REMOVE O WARNING QUE É APRESENTADO QUANDO INICIALIZA A SESSÃO DO Spark\n",
    "\n",
    "# 24/06/22 04:04:51 WARN Utils: Your hostname, edsojor resolves to a loopback address: 127.0.1.1; using 192.168.15.6 instead (on interface wlo1)\n",
    "# 24/06/22 04:04:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
    "# Setting default log level to \"WARN\".\n",
    "# To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "# 24/06/22 04:04:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae6bf7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.sparkContext.setLogLevel(\"ERROR\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "899618ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path =\"data/sbom_27082024_040547.json\"\n",
    "paths = read_files(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99887be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_spark_dataframes = read_files_json_spark(spark_session, paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b7da0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-------------------------------------+----------------------+-----------------------+\n",
      "|versao_biblioteca|licenca_biblioteca|nome_repositorio                     |gerenciador_biblioteca|nome_biblioteca        |\n",
      "+-----------------+------------------+-------------------------------------+----------------------+-----------------------+\n",
      "|1.0.3            |MIT               |data-prepper                         |npm                   |has                    |\n",
      "|0.2.6            |MIT               |data-prepper                         |npm                   |bs-logger              |\n",
      "|29.7.0           |MIT               |data-prepper                         |npm                   |jest-runner            |\n",
      "|1.0.0            |MIT               |opensearch-dashboards-functional-test|npm                   |prettier-linter-helpers|\n",
      "|2.2.3            |MIT               |data-prepper                         |npm                   |json5                  |\n",
      "+-----------------+------------------+-------------------------------------+----------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+------------------+-------------------------------------+----------------------+-----------------------+\n",
      "|versao_biblioteca|licenca_biblioteca|nome_repositorio                     |gerenciador_biblioteca|nome_biblioteca        |\n",
      "+-----------------+------------------+-------------------------------------+----------------------+-----------------------+\n",
      "|1.0.3            |MIT               |data-prepper                         |npm                   |has                    |\n",
      "|0.2.6            |MIT               |data-prepper                         |npm                   |bs-logger              |\n",
      "|29.7.0           |MIT               |data-prepper                         |npm                   |jest-runner            |\n",
      "|1.0.0            |MIT               |opensearch-dashboards-functional-test|npm                   |prettier-linter-helpers|\n",
      "|2.2.3            |MIT               |data-prepper                         |npm                   |json5                  |\n",
      "+-----------------+------------------+-------------------------------------+----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_dataframe = transform_spark_frame(list_spark_dataframes)\n",
    "spark_dataframe.show(truncate=False, n=5)\n",
    "teste = spark_dataframe.limit(5)\n",
    "teste.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84de293a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- versao_biblioteca: string (nullable = true)\n",
      " |-- licenca_biblioteca: string (nullable = true)\n",
      " |-- nome_repositorio: string (nullable = true)\n",
      " |-- gerenciador_biblioteca: string (nullable = true)\n",
      " |-- nome_biblioteca: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_dataframe.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbd0c900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3064"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_dataframe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3768bdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- superior_emp_id: long (nullable = true)\n",
      " |-- year_joined: string (nullable = true)\n",
      " |-- emp_dept_id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
      "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
      "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
      "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
      "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
      "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
      "+------+--------+---------------+-----------+-----------+------+------+\n",
      "\n",
      "root\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- dept_id: long (nullable = true)\n",
      "\n",
      "+---------+-------+\n",
      "|dept_name|dept_id|\n",
      "+---------+-------+\n",
      "|Finance  |10     |\n",
      "|Marketing|20     |\n",
      "|Sales    |30     |\n",
      "|IT       |40     |\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
    "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
    "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
    "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
    "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
    "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
    "  ]\n",
    "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
    "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
    "\n",
    "empDF = spark_session.createDataFrame(data=emp, schema = empColumns)\n",
    "empDF.printSchema()\n",
    "empDF.show(truncate=False)\n",
    "\n",
    "dept = [(\"Finance\",10), \\\n",
    "    (\"Marketing\",20), \\\n",
    "    (\"Sales\",30), \\\n",
    "    (\"IT\",40) \\\n",
    "  ]\n",
    "deptColumns = [\"dept_name\",\"dept_id\"]\n",
    "deptDF = spark_session.createDataFrame(data=dept, schema = deptColumns)\n",
    "deptDF.printSchema()\n",
    "deptDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20a414b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "|6     |Brown|2              |2010       |50         |      |-1    |\n",
      "+------+-----+---------------+-----------+-----------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Syntax\n",
    "# join(self, other, on=None, how=None)\n",
    "\n",
    "result_left_anti = empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id, how='left_anti')\n",
    "result_left_anti.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af186565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_insert(left_frame, right_frame, how_:str='left_anti'):\n",
    "    result = left_frame.join(right_frame, on='id_check', how=how_)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "93efcde0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|desc_order|\n",
      "+---+----------+\n",
      "|  0|         2|\n",
      "|  1|         3|\n",
      "|  2|         4|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "df = spark_session.range(3)\n",
    "w = Window.orderBy(df['id'].asc())\n",
    "teste = df.withColumn(\"desc_order\", F.row_number().over(w)+1)\n",
    "teste.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eed973e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=0\n",
      "b=3\n",
      "c=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def soma(a, b, c):\n",
    "    print(f'{a=}')\n",
    "    print(f'{b=}')\n",
    "    print(f'{c=}')\n",
    "    \n",
    "    return a + b\n",
    "\n",
    "soma(b=3, c=1, a=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5a061bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb323265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# desligar/parar a sessão spark \n",
    "# spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39407693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from awsglue.transforms import *\n",
    "# from awsglue.utils import getResolvedOptions\n",
    "# from pyspark.context import SparkContext\n",
    "# from awsglue.context import GlueContext\n",
    "# from awsglue.job import Job\n",
    "  \n",
    "# sc = SparkContext.getOrCreate()\n",
    "# glueContext = GlueContext(sc)\n",
    "# spark = glueContext.spark_session\n",
    "# job = Job(glueContext)\n",
    "\n",
    "\n",
    "# s3output = glueContext.getSink(\n",
    "#   path=\"s3://bucket_name/folder_name\",\n",
    "#   connection_type=\"s3\",\n",
    "#   updateBehavior=\"UPDATE_IN_DATABASE\",\n",
    "#   partitionKeys=[],\n",
    "#   compression=\"snappy\",\n",
    "#   enableUpdateCatalog=True,\n",
    "#   transformation_ctx=\"s3output\",\n",
    "# )\n",
    "# s3output.setCatalogInfo(\n",
    "#   catalogDatabase=\"demo\", catalogTableName=\"populations\"\n",
    "# )\n",
    "# s3output.setFormat(\"glueparquet\")\n",
    "# s3output.writeFrame(DyF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb58932",
   "metadata": {},
   "source": [
    "## Existem três principais maneiras de desenvolver trabalhos do AWS Glue localmente:\n",
    "\n",
    "1. **AWS Glue ETL Sdk**: Permite que os desenvolvedores escrevam scripts usando a API do Python, disponibilizando uma experiência de desenvolvimento familiar e flexível.\n",
    "\n",
    "2. **AWS Glue PySpark**: Utiliza o PySpark, que é a API Python para o Apache Spark, permitindo que os desenvolvedores criem trabalhos complexos de transformação de dados usando o Spark.\n",
    "\n",
    "3. **AWS Glue DataBrew**: Oferece uma interface visual que permite aos usuários criar e modificar transformações de dados sem a necessidade de escrever código.\n",
    "\n",
    "Essas três maneiras oferecem opções para diferentes preferências e necessidades de desenvolvimento.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
